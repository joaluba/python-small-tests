{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import torchaudio\n",
    "from IPython.display import Audio\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import soundfile as sf\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS NOTEBOOK: \n",
    "\n",
    "# Sometimes we need to crop the signal making sure that there is no silece in the cropped audio. \n",
    "# To do that we can compute signal energy and decide based on that. \n",
    "# One way of computing signal energy is by computing convolution of a squared signal with a flat kernel. \n",
    "# It is equivalent to a moving average filter, where the convolution kernel size defines the length of the\n",
    "# averaging window. However, for long signals convolution is quite expensive, and for cropping audio we \n",
    "# can use a less precise approach. So below, I am testing how to use strided convolution in pytorch to \n",
    "# pick a window with more or less the highest energy. This is equivalent to computing moving average with \n",
    "# a certain step-size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve_torchaudio(sig, ir):\n",
    "    sig_tensor = torch.tensor(sig, dtype=torch.float32)\n",
    "    ir_tensor = torch.tensor(ir, dtype=torch.float32)\n",
    "    return torchaudio.functional.convolve(sig_tensor, ir_tensor)\n",
    "\n",
    "def convolve_torch2(sig, ir,device=\"cpu\",stride=1000):\n",
    "    # sig - audio signal to be convolved (in conv1d terms - input tensor)\n",
    "    # ir - impulse response to colvolve the signal with (in conv1d terms - convolution kernel)\n",
    "\n",
    "    # torch.nn.Conv1d performs autocorrelation operation. To make it the actual convolution, the \n",
    "    # kernel has to be flipped. We have to make a copy of the flipped variable, otherwise we will get the \n",
    "    # error saying that the stride in numpy array is negative, and tensors with negative strides are not supported. \n",
    "    ir_flipped=ir[::-1].copy()\n",
    "    # prepare the shape of the sig and ir so that it matches what Conv1d expects:\n",
    "    sig_tensor = torch.tensor(sig, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Shape: (batch_size, in_channels, input_length)\n",
    "    ir_tensor = torch.tensor(ir_flipped.copy(), dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Shape: (batch_size, in_channels, input_length)\n",
    "    # create padding that allows for obtaining a \"full\" convolution, where the size of the \n",
    "    # convolved output signal is N+M-1 , where N=len(sig), M=len(ir)\n",
    "    full_padding = len(ir_tensor[0][0]) - 1\n",
    "    # define convolutional layer:\n",
    "    conv_layer = torch.nn.Conv1d(in_channels=1, out_channels=1, kernel_size=ir_tensor.shape[2], stride=stride, padding=full_padding, bias=False)\n",
    "    # move data to device\n",
    "    sig_tensor=sig_tensor.to(device)\n",
    "    ir_tensor=ir_tensor.to(device)\n",
    "    # Set the weights of the convolutional layer (kernel) to be the flipped impulse response:\n",
    "    with torch.no_grad():\n",
    "        conv_layer.weight.data = ir_tensor\n",
    "        \n",
    "    return conv_layer(sig_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load audio:\n",
    "audio, fs = sf.read('audios/speech_VCTK_4_sentences.wav')\n",
    "audio=audio[0:5*fs]\n",
    "# (D,) -> (1xD)\n",
    "audio=np.reshape(audio, (1, -1))\n",
    "\n",
    "# create convolutional kernel consisting equal weights\n",
    "# convolution with a flat kernel correspond to a moving average\n",
    "window_lengths=[0.2, 0.5,1,3]\n",
    "for L in window_lengths:\n",
    "    kernel=  torch.ones(1,round(L*fs))/round(L*fs)\n",
    "    # convolve\n",
    "    start=time.time()\n",
    "    envelope = convolve_torchaudio(torch.tensor(audio**2),kernel)\n",
    "    \n",
    "    end=time.time()\n",
    "    print(\"for windowlength \"+str(L)+ \"the convolution time is: \" +str(end-start))\n",
    "\n",
    "    plt.figure(figsize=(5,1))\n",
    "    plt.subplot(1,2,1);plt.plot(audio.T);plt.title(\"audio signal\")\n",
    "    plt.subplot(1,2,2);plt.plot(envelope.T);plt.title(\"envelope\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually, for envelope extraction we don't need to know the moving average for every audio sample, \n",
    "# we can check the segments energy with a higher step size (stride). To do that we can use the \n",
    "# pytorch 1d convolution. From previous analysis I know that the fastest pytorch convolution, which \n",
    "# allows to specify the stride is the nn.Conv1d function, so I can use it here: \n",
    "\n",
    "# load audio:\n",
    "audio, fs = sf.read('audios/speech_VCTK_4_sentences.wav')\n",
    "audio=audio[0:5*fs]\n",
    "\n",
    "# create convolutional kernel consisting equal weights\n",
    "# convolution with a flat kernel correspond to a moving average\n",
    "\n",
    "step_len=1 # averaging step-size in seconds\n",
    "win_len= 2 # averaging window in seconds\n",
    "stride=round(step_len*fs)\n",
    "kernel= np.ones((round(win_len*fs),))/kernel.shape[0]\n",
    "kernel_size=kernel.shape[0]\n",
    "\n",
    "# convolve with a specific stride:\n",
    "start=time.time()\n",
    "sparse_envelope = convolve_torch2(audio**2,kernel,stride=stride).detach().numpy().squeeze(0).squeeze(0)\n",
    "audio_padded=np.concatenate((np.zeros(kernel_size-1),audio, np.zeros(kernel_size-1)))\n",
    "envelope_padded=np.zeros(audio_padded.shape)\n",
    "envelope_padded[0:-kernel.shape[0]:stride]=sparse_envelope\n",
    "stop=time.time()\n",
    "print(\"Execution time for convolution:\"+ str(stop-start))\n",
    "\n",
    "# compute moving average:\n",
    "start=time.time()\n",
    "envelope_padded_check=np.zeros(audio_padded.shape)\n",
    "for i in range(0,envelope_padded_check.shape[0]-kernel.shape[0],stride):\n",
    "    envelope_padded_check[i]=np.mean(audio_padded[i:i+kernel.shape[0]]**2)\n",
    "stop=time.time()\n",
    "print(\"Execution time for moving average:\"+ str(stop-start))\n",
    "\n",
    "\n",
    "# check if the result of the convolution is equivalent to \n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(3,1,1);plt.plot(audio_padded);plt.title(\"audio signal\")\n",
    "plt.subplot(3,1,2);plt.plot(envelope_padded,color=\"red\"),plt.title(\"energy computed using Conv1d with stride\")\n",
    "plt.subplot(3,1,3);plt.plot(envelope_padded_check,color=\"red\"),plt.title(\"energy computed using moving average window\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- AUDIO ENERGY-BASED CROPPING FUNCTION USING MOVING AVERAGE -------\n",
    "# Below a function that computes moving average of a tensor in a for loop, \n",
    "# with a given window lenght and step size, and later picks the window with\n",
    "# the highest energy\n",
    "\n",
    "# load audio:\n",
    "audio, fs = sf.read('audios/speech_VCTK_4_sentences.wav')\n",
    "audio=audio[0:3*fs]\n",
    "# desired cropped audio lenght in seconds:\n",
    "L_win_sec=2\n",
    "# stepsize for moving average\n",
    "stride_s=L_win_sec/6\n",
    "\n",
    "# np -> pytorch\n",
    "audio_torch=torch.tensor(audio).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "def ma_based_crop_torch(audio,fs,L_win_sec,stride_s):\n",
    "    L_audio_smpl=audio.shape[2] # Shape: (batch_size, in_channels, input_length)\n",
    "    L_win_smpl=int(L_win_sec*fs)\n",
    "    stride_smpl=int(stride_s*fs)\n",
    "    energy=torch.zeros(L_audio_smpl)\n",
    "    for i in range(0,L_audio_smpl-(L_win_smpl-1),stride_smpl):\n",
    "        energy[i]=torch.mean(audio[:,:,i:i+L_win_smpl]**2)\n",
    "    # find the window with the highest energy\n",
    "    idx_start=int(torch.argmax(energy))\n",
    "    idx_end=idx_start+L_win_smpl\n",
    "    audio_crop=audio[:,:,idx_start:idx_end]\n",
    "    return audio_crop, energy, idx_start, idx_end\n",
    "\n",
    "start=time.time()\n",
    "audio_crop_torch, energy, idx_start, idx_end=ma_based_crop_torch(audio_torch,fs,L_win_sec,stride_s)\n",
    "stop=time.time()\n",
    "print(\"Execution time for pytorch moving average audio cropping function:\"+ str(stop-start))\n",
    "\n",
    "audio_crop=audio_crop_torch[0,0,:].numpy()\n",
    "audio_crop_padded=np.zeros(audio.shape)\n",
    "audio_crop_padded[idx_start:idx_end]=audio_crop\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(3,1,1);plt.plot(audio);plt.title(\"audio\")\n",
    "plt.subplot(3,1,2);plt.plot(energy);plt.title(\"moving average without padding - energy estimate\")\n",
    "plt.subplot(3,1,3);plt.plot(audio_crop_padded);plt.title(\"audio cropped based on energy\")\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- AUDIO ENERGY-BASED CROPPING FUNCTION USING CONVOLUTION -------\n",
    "# Below a function that computes moving average of a tensor using strided convolution, \n",
    "# with a given window lenght (kernel size) and step size (stride), and later picks the \n",
    "# window with the highest energy\n",
    "\n",
    "# load audio:\n",
    "audio, fs = sf.read('audios/speech_VCTK_4_sentences.wav')\n",
    "audio=audio[0:3*fs]\n",
    "# desired cropped audio lenght in seconds:\n",
    "L_win_sec=2\n",
    "# stepsize for moving average\n",
    "stride_s=L_win_sec/6\n",
    "\n",
    "# np -> pytorch\n",
    "audio_torch=torch.tensor(audio, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "def conv_based_crop_torch(audio,fs,L_win_sec,stride_s):\n",
    "    L_audio_smpl=audio.shape[2] # Shape: (batch_size, in_channels, input_length)\n",
    "    L_win_smpl=int(L_win_sec*fs)\n",
    "    stride_smpl=int(stride_s*fs)\n",
    "    kernel= torch.ones((L_win_smpl,),dtype=torch.float32)/L_win_smpl\n",
    "    kernel=kernel.unsqueeze(0).unsqueeze(0)\n",
    "    conv_layer=torch.nn.Conv1d(in_channels=1, out_channels=1, kernel_size=L_win_smpl, stride=stride_smpl, bias=False)\n",
    "    with torch.no_grad():\n",
    "        conv_layer.weight.data = kernel   \n",
    "    out_layer=conv_layer(audio**2)\n",
    "    energy=torch.zeros(L_audio_smpl)\n",
    "    energy[0:-(L_win_smpl-1):stride_smpl]=out_layer[0,0,:].detach()\n",
    "    # find the window with the highest energy\n",
    "    idx_start=int(torch.argmax(energy))\n",
    "    idx_end=idx_start+L_win_smpl\n",
    "    audio_crop=audio[:,:,idx_start:idx_end]\n",
    "    return audio_crop, energy, idx_start, idx_end\n",
    "\n",
    "start=time.time()\n",
    "audio_crop_torch, energy, idx_start, idx_end=conv_based_crop_torch(audio_torch,fs,L_win_sec,stride_s)\n",
    "stop=time.time()\n",
    "print(\"Execution time for pytorch moving average audio cropping function:\"+ str(stop-start))\n",
    "\n",
    "audio_crop=audio_crop_torch[0,0,:].numpy()\n",
    "audio_crop_padded=np.zeros(audio.shape)\n",
    "audio_crop_padded[idx_start:idx_end]=audio_crop\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(3,1,1);plt.plot(audio);plt.title(\"audio\")\n",
    "plt.subplot(3,1,2);plt.plot(energy);plt.title(\"convolution without padding - energy estimate\")\n",
    "plt.subplot(3,1,3);plt.plot(audio_crop_padded);plt.title(\"audio cropped based on energy\")\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ha_enhancement_experiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
