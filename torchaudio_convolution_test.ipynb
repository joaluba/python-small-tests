{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import torchaudio\n",
    "from IPython.display import Audio\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import soundfile as sf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS NOTEBOOK: \n",
    "\n",
    "# What is exactly the relation between the audio convolution used to, for example, add reverb to a signal,\n",
    "# and the pytorch Conv1d used in a convolutional neural network?\n",
    "# I wanted to understand this, because sometimes it might be needed to do audio convolution on gpu and \n",
    "# for that pytorch functions have to be used. For that a torchaudio function is good cause it can be used just as\n",
    "# a typical numpy or scipy convolution. \n",
    "# It can also important to know this if we want to have a trainable impulse response in a convolutional neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the convolution functions\n",
    "def convolve_scipy(sig, ir):\n",
    "    return signal.fftconvolve(sig, ir,mode=\"full\")\n",
    "\n",
    "def convolve_numpy(sig, ir):\n",
    "    return np.convolve(sig, ir,mode=\"full\")\n",
    "\n",
    "def convolve_torchaudio(sig, ir, device=\"cpu\"):\n",
    "    sig_tensor = torch.tensor(sig, dtype=torch.float32).to(device)\n",
    "    ir_tensor = torch.tensor(ir, dtype=torch.float32).to(device)\n",
    "    return torchaudio.functional.convolve(sig_tensor, ir_tensor)\n",
    "\n",
    "def convolve_torch1(sig, ir,device=\"cpu\"):\n",
    "    # sig - audio signal to be convolved (in conv1d terms - input tensor)\n",
    "    # ir - impulse response to colvolve the signal with (in conv1d terms - convolution kernel)\n",
    "\n",
    "    # torch.nn.functional.conv1d performs autocorrelation operation. To make it the actual convolution, the \n",
    "    # kernel has to be flipped. We have to make a copy of the flipped variable, otherwise we will get the \n",
    "    # error saying that the stride in numpy array is negative, and tensors with negative strides are not supported: \n",
    "    ir_flipped=ir[::-1].copy()\n",
    "    # prepare the shape of the sig and ir so that it matches what conv1d expects:\n",
    "    sig_tensor = torch.tensor(sig, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Shape: (batch_size, in_channels, input_length)\n",
    "    ir_tensor = torch.tensor(ir_flipped, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Shape: (batch_size, in_channels, input_length)\n",
    "    # create padding that allows for obtaining a \"full\" convolution, where the size of the \n",
    "    # convolved output signal is N+M-1 , where N=len(sig), M=len(ir)\n",
    "    full_padding = len(ir_tensor[0][0]) - 1\n",
    "    sig_tensor = torch.nn.functional.pad(sig_tensor, (full_padding, full_padding), mode='constant', value=0)\n",
    "    return torch.nn.functional.conv1d(sig_tensor, ir_tensor)\n",
    "\n",
    "def convolve_torch2(sig, ir, device=\"cpu\"):\n",
    "    # sig - audio signal to be convolved (in conv1d terms - input tensor)\n",
    "    # ir - impulse response to colvolve the signal with (in conv1d terms - convolution kernel)\n",
    "\n",
    "    # torch.nn.Conv1d performs autocorrelation operation. To make it the actual convolution, the \n",
    "    # kernel has to be flipped. We have to make a copy of the flipped variable, otherwise we will get the \n",
    "    # error saying that the stride in numpy array is negative, and tensors with negative strides are not supported. \n",
    "    ir_flipped=ir[::-1].copy()\n",
    "    # prepare the shape of the sig and ir so that it matches what Conv1d expects:\n",
    "    sig_tensor = torch.tensor(sig, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)  # Shape: (batch_size, in_channels, input_length)\n",
    "    ir_tensor = torch.tensor(ir_flipped.copy(), dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device) # Shape: (batch_size, in_channels, input_length)\n",
    "    # create padding that allows for obtaining a \"full\" convolution, where the size of the \n",
    "    # convolved output signal is N+M-1 , where N=len(sig), M=len(ir)\n",
    "    full_padding = len(ir_tensor[0][0]) - 1\n",
    "    # define convolutional layer:\n",
    "    conv_layer = torch.nn.Conv1d(in_channels=1, out_channels=1, kernel_size=ir_tensor.shape[2], stride=1, padding=full_padding, bias=False)\n",
    "    # Set the weights of the convolutional layer (kernel) to be the flipped impulse response:\n",
    "    with torch.no_grad():\n",
    "        conv_layer.weight.data = ir_tensor\n",
    "        \n",
    "    return conv_layer(sig_tensor)\n",
    "\n",
    "# Other convolution tests:\n",
    "# https://laurentperrinet.github.io/sciblog/posts/2017-09-20-the-fastest-2d-convolution-in-the-world.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate or load sample audio signals:\n",
    "\n",
    "# # generate:\n",
    "# np.random.seed(0)\n",
    "# signal1 = np.random.rand(1000)\n",
    "# signal2 = np.random.rand(500)\n",
    "\n",
    "# load:\n",
    "signal1, fs1 = sf.read('audios/speech_VCTK_4_sentences.wav')\n",
    "signal1=signal1[1:2*fs1]\n",
    "signal2, fs2 = sf.read('audios/rir_ACE_lecture_hall.wav')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the convolution functions\n",
    "start=time.time()\n",
    "result_scipy = convolve_scipy(signal1, signal2)\n",
    "end_scipy = time.time()\n",
    "result_numpy = convolve_numpy(signal1, signal2)\n",
    "end_numpy= time.time()\n",
    "result_torchaudio = convolve_torchaudio(signal1, signal2).detach().numpy()\n",
    "end_torchaudio= time.time()\n",
    "result_torch1 = convolve_torch1(signal1, signal2).detach().numpy().squeeze(0).squeeze(0)\n",
    "end_torch1=time.time()\n",
    "result_torch2 = convolve_torch2(signal1, signal2).detach().numpy().squeeze(0).squeeze(0)\n",
    "end_torch2=time.time()\n",
    "\n",
    "print(\"scipy convolution time: \" +str(end_scipy-start))\n",
    "print(\"numpy convolution time: \" +str(end_numpy-end_scipy))\n",
    "print(\"torchaudio convolution time: \" +str(end_torchaudio-end_numpy))\n",
    "print(\"F.conv1d convolution time: \" +str(end_torch1- end_torchaudio))\n",
    "print(\"nn.Conv1d convolution time: \" +str(end_torch2- end_torch1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{result_scipy.shape=}\")\n",
    "print(f\"{result_numpy.shape=}\")\n",
    "print(f\"{result_torchaudio.shape=}\")\n",
    "print(f\"{result_torch1.shape=}\")\n",
    "print(f\"{result_torch2.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.allclose(result_scipy, result_torchaudio))\n",
    "print(np.allclose(result_numpy, result_torchaudio))\n",
    "print(np.allclose(result_numpy, result_torch2))\n",
    "print(np.allclose(result_numpy, result_scipy))\n",
    "print(np.allclose(result_numpy, result_torch1))\n",
    "print(np.allclose(result_torch1, result_scipy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(5,1,1);plt.plot(result_numpy);plt.title(\"numpy\")\n",
    "plt.subplot(5,1,2);plt.plot(result_scipy);plt.title(\"scipy\")\n",
    "plt.subplot(5,1,3);plt.plot(result_torchaudio);plt.title(\"torchaudio\")\n",
    "plt.subplot(5,1,4);plt.plot(result_torch1);plt.title(\"functional.conv1d\")\n",
    "plt.subplot(5,1,5);plt.plot(result_torch2);plt.title(\"nn.Conv1d\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(result_torch2,rate=48000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
